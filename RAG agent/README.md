# ğŸ“š Amazon Annual Report RAG Chatbot

This repository contains two **n8n** workflows that together implement a **Retrieval-Augmented Generation (RAG)** system for querying Amazonâ€™s Annual Report.  
Using **Pinecone**, **Ollama embeddings**, and **Google Gemini**, you can upload a PDF report, store it as vector embeddings, and interact with it via a chatbot.

---

## ğŸ” What is RAG?
**Retrieval-Augmented Generation** combines:
1. **Retrieval** â€“ Fetch relevant document chunks from a vector database.
2. **Augmentation** â€“ Add these chunks to the LLM prompt.
3. **Generation** â€“ Use an LLM to produce a final, context-aware answer.

This project uses:
- **DriveToVector** â†’ Data ingestion & embedding.
- **VectorChat** â†’ Querying & chat interface.

---

## ğŸ“‚ Workflows

### **1ï¸âƒ£ DriveToVector** â€“ Data Ingestion Workflow
**Purpose:**  
Takes a PDF (Amazonâ€™s Annual Report) from Google Drive, processes it into vector embeddings, and stores them in Pinecone for later retrieval.

**Main Steps:**
1. **Manual Trigger** â€“ Start workflow manually.
2. **Google Drive Download** â€“ Downloads the PDF from a specified file ID.
3. **Document Loader & Splitter** â€“ Splits text into manageable chunks using recursive character splitting.
4. **Ollama Embeddings** â€“ Uses `nomic-embed-text:latest` to generate embeddings.
5. **Pinecone Vector Store** â€“ Inserts embeddings into the `amazon` index, namespace `AMAZON`.

---

**Requirements:**
- n8n (latest)
- Google Drive API credentials
- Pinecone API credentials
- Ollama running locally (`ollama pull nomic-embed-text:latest`)

<br>

### **2ï¸âƒ£ VectorChat** â€“ Chatbot Workflow
**Purpose:**  
Enables interactive question-answering over the stored document using RAG.

**Main Steps:**
1. **Chat Trigger** â€“ Listens for incoming user messages.
2. **Amazon Agent** â€“ Orchestrates retrieval and response generation.
3. **Memory Buffer** â€“ Maintains conversation context.
4. **Embedding & Retrieval** â€“ Converts queries into embeddings, fetches relevant vectors from Pinecone.
5. **Google Gemini** â€“ Generates a natural language answer, augmented with retrieved document chunks.

**Requirements:**
- n8n (latest)
- Pinecone API credentials
- Ollama running locally (`ollama pull nomic-embed-text:latest`)
- Google Gemini (PaLM) API credentials

---

## âš™ï¸ Setup Instructions

1. **Import Workflows**  
   - In n8n: `Workflows â†’ Import from File` â†’ Select `DriveToVector.json` and `VectorChat.json`.

2. **Configure Credentials**  
   - **Google Drive**: Set file ID for your PDF.  
   - **Pinecone**: Add API key & index name (`amazon`).  
   - **Ollama**: Ensure embeddings model is installed.  
   - **Google Gemini**: Add API key.

3. **Run DriveToVector First**  
   - Populates Pinecone with PDF data.

4. **Run VectorChat**  
   - Chat and query the stored document.

---

## ğŸ’¡ Example

**User:**  
> What was Amazonâ€™s total revenue in 2024?

**Bot:**  
> Amazonâ€™s total revenue in 2024 was X, driven primarily by AWS and e-commerce sales...

---

## ğŸ“œ License
MIT License â€” feel free to use, modify, and distribute.

---

**Author:** Shivasubrahmanya K C  
**Created with:** Using n8n, Pinecone, Ollama, and Google Gemini
